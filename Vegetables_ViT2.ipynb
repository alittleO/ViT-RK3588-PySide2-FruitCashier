{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vegatables: Image Classification\n",
    "\n",
    "Dataset Source: https://www.kaggle.com/datasets/misrakahmed/vegetable-image-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install Necessary Libraries Not Already Installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (2.18.0)\n",
      "Requirement already satisfied: transformers in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (4.39.1)\n",
      "Requirement already satisfied: tensorboard in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (2.16.2)\n",
      "Requirement already satisfied: evaluate in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (0.4.1)\n",
      "Requirement already satisfied: filelock in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from datasets) (1.2.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from datasets) (0.22.1)\n",
      "Requirement already satisfied: packaging in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from tensorboard) (1.62.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from tensorboard) (3.6)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from tensorboard) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from tensorboard) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from tensorboard) (3.0.1)\n",
      "Requirement already satisfied: responses<0.19 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard) (7.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.18.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets transformers tensorboard evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "# 设置环境变量，禁用 tokenizers 库的并行处理。\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# 导入用于图像处理和绘制的库。\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "import PIL.Image\n",
    "\n",
    "# 导入 tqdm，用于在循环或迭代过程中显示进度条。\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 导入 numpy 和 pandas，用于数值计算和数据操作。\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 导入 datasets 库，用于处理数据集和加载数据。\n",
    "import datasets\n",
    "from datasets import load_dataset, Image, load_metric\n",
    "\n",
    "# 导入 transformers 库，用于使用预训练模型和相关工具。\n",
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "\n",
    "# 导入 PyTorch，用于深度学习操作。\n",
    "import torch\n",
    "\n",
    "# 导入 evaluate 库，用于模型评估指标。\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Display Versions of Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Python: 3.9.19\n",
      "         NumPy: 1.26.4\n",
      "        Pandas: 1.2.4\n",
      "      Datasets: 2.18.0\n",
      "  Transformers: 4.39.1\n",
      "         Torch: 2.2.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Python:\".rjust(15), sys.version[0:6])\n",
    "print(\"NumPy:\".rjust(15), np.__version__)\n",
    "print(\"Pandas:\".rjust(15), pd.__version__)\n",
    "print(\"Datasets:\".rjust(15), datasets.__version__)\n",
    "print(\"Transformers:\".rjust(15), transformers.__version__)\n",
    "print(\"Torch:\".rjust(15), torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ingest Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8ecf0a0f314af4bf6c3e36dfca490a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98942f044ecb43bf8c0e20f1b79821fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68a46f684434fe5b0402e4b37b7d7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/359 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset\n",
      "Dataset({\n",
      "    features: ['image', 'label'],\n",
      "    num_rows: 3115\n",
      "})\n",
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2218x2216 at 0x74E48BB0FA60>, 'label': 0}\n",
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1200x1806 at 0x74E48BB0F850>, 'label': 35}\n",
      "Testing Dataset\n",
      "Dataset({\n",
      "    features: ['image', 'label'],\n",
      "    num_rows: 359\n",
      "})\n",
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2218x2216 at 0x74E48BB0FE80>, 'label': 0}\n",
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x1170 at 0x74E48BB229A0>, 'label': 35}\n"
     ]
    }
   ],
   "source": [
    "# 加载 imagefolder 数据集。指定数据集的路径和是否丢弃标签。\n",
    "dataset = load_dataset(\"imagefolder\", \n",
    "                        data_dir=\"../Data/fruit-and-vegetable-image-recognition\", \n",
    "                        drop_labels=False)\n",
    "\n",
    "# 打印训练数据集的信息。\n",
    "print(\"Training Dataset\")\n",
    "print(dataset['train'])       # 打印训练集的整体信息。\n",
    "print(dataset['train'][0])    # 打印训练集的第一个样本。\n",
    "print(dataset['train'][-1])   # 打印训练集的最后一个样本。\n",
    "\n",
    "# 打印测试数据集的信息。\n",
    "print(\"Testing Dataset\")\n",
    "print(dataset['test'])        # 打印测试集的整体信息。\n",
    "print(dataset['test'][0])     # 打印测试集的第一个样本。\n",
    "print(dataset['test'][-1])    # 打印测试集的最后一个样本。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Display Grid of Examples From Each Class to Gain Better Picture of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这个函数的目的是在数据集的每个类别中显示几个示例图片，\n",
    "#并在每个图片上标注其对应的类别标签。这对于可视化和理解数据集的内容非常有用。\n",
    "def show_grid_of_examples(ds, \n",
    "                          seed: int = 42, \n",
    "                          examples_per_class: int = 3, \n",
    "                          size=(350, 350)):\n",
    "    '''\n",
    "    该函数在数据集的每个类别中显示几个示例图片。\n",
    "    '''\n",
    "    w, h = size  # 设置每个图片的宽度和高度。\n",
    "    labels = ds['train'].features['label'].names  # 获取所有类别的标签名称。\n",
    "    grid = PIL.Image.new(mode='RGB', size=(examples_per_class * w, len(labels) * h))  # 创建一个新的空白图像用于放置所有示例图片。\n",
    "    draw = ImageDraw.Draw(grid)  # 创建一个用于绘制文本的对象。\n",
    "    font = ImageFont.truetype(\"MiSans-Normal.ttf\", 24)  # 设置文本的字体和大小。\n",
    "    \n",
    "    for label_id, label in enumerate(labels):  # 遍历每个类别的标签。\n",
    "        # 过滤出单个标签的数据集，打乱它，然后选取一些样本。\n",
    "        ds_slice = ds['train'] \\\n",
    "                    .filter(lambda ex: ex['label'] == label_id) \\\n",
    "                    .shuffle(seed) \\\n",
    "                    .select(range(examples_per_class))\n",
    "        \n",
    "        # 在一行中绘制这个标签的示例图片。\n",
    "        for i, example in enumerate(ds_slice):\n",
    "            image = example['image']  # 获取图片。\n",
    "            idx = examples_per_class * label_id + i  # 计算图片在网格中的位置。\n",
    "            box = (idx % examples_per_class * w, idx // examples_per_class * h)  # 计算图片在网格中的坐标。\n",
    "            grid.paste(image.resize(size), box=box)  # 将图片粘贴到网格中。\n",
    "            draw.text(box, label, (255, 255, 255), font=font, dill=(0,0,255,1.0))  # 在图片上绘制标签文本。\n",
    "    \n",
    "    return grid  # 返回包含所有示例图片的网格图像。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#显示数据集中每个类别的几个示例图片\n",
    "#show_grid_of_examples(dataset, seed=42, examples_per_class=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remember to Install git lfs & Enter HuggingFace Access Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git LFS initialized.\r\n"
     ]
    }
   ],
   "source": [
    "# Enter Huggingface Access Token\n",
    "\n",
    "!git lfs install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic Values/Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置模型的检查点，这里使用的是 Google 提供的 Vision Transformer\n",
    "#(ViT) 基础模型，图片大小为 224x224，预训练于 ImageNet21k 数据集。\n",
    "MODEL_CKPT = 'google/vit-base-patch16-224-in21k'\n",
    "# 设置训练的总轮数。\n",
    "NUM_OF_EPOCHS = 10\n",
    "\n",
    "# 设置学习率。\n",
    "LEARNING_RATE = 2e-4\n",
    "# 设置训练的步数。\n",
    "STEPS = 100\n",
    "\n",
    "# 设置批处理大小。\n",
    "BATCH_SIZE = 16\n",
    "# 设置设备，这里使用的是 Metal Performance Shaders (MPS) 设备，\n",
    "#适用于在 Apple 设备上进行神经网络计算。\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "\n",
    "# 设置报告输出方式，这里使用 TensorBoard 进行可视化展示。\n",
    "REPORTS_TO = 'tensorboard'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load ViT Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''使用 ViTFeatureExtractor.from_pretrained 方法从预训练的\n",
    "Vision Transformer (ViT) 模型中加载特征提取器。MODEL_CKPT 是模型的检查点，\n",
    "它指定了预训练模型的来源。在你之前的代码中，MODEL_CKPT 被设置为\n",
    "'google/vit-base-patch16-224-in21k'，这意味着你将从 Hugging Face \n",
    "模型库中加载 Google 提供的 ViT 基础模型，它的图片大小为 224x224，\n",
    "预训练于 ImageNet21k 数据集。\n",
    "\n",
    "这个特征提取器用于将图像数据预处理成模型所需的格式。例如，\n",
    "它会对图像进行大小调整、归一化等操作。你可以使用这个特征提取器来处理你的图像数据，\n",
    "然后将处理后的数据输入到 ViT 模型中进行预测或训练。'''\n",
    "from transformers import ViTImageProcessor\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(MODEL_CKPT)\n",
    "\n",
    "#feature_extractor = ViTFeatureExtractor.from_pretrained(MODEL_CKPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(sample_batch):\n",
    "    # 将一个包含 PIL 图像的列表转换为像素值\n",
    "    # feature_extractor 是之前定义的用于提取特征的对象\n",
    "    # 这里使用列表推导式 [x for x in sample_batch['image']] 获取图像列表\n",
    "    # return_tensors=\"pt\" 指定返回的张量类型为 PyTorch 张量\n",
    "    inputs = feature_extractor([x.convert(\"RGB\") for x in sample_batch['image']], return_tensors=\"pt\")\n",
    "    # 将一个包含 PIL 图像的列表转换为 RGB 格式的像素值\n",
    "    \n",
    "    # 准备标签\n",
    "    # 将样本批次中的标签添加到输入字典中\n",
    "    inputs['labels'] = sample_batch['label']\n",
    "    \n",
    "    # 返回处理后的输入字典，包含像素值和标签\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply Transform Function to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用之前定义的 transform 函数对数据集进行预处理\n",
    "# dataset 是之前加载的数据集，包含图像和标签\n",
    "# with_transform 方法将 transform 函数应用于数据集中的每个样本\n",
    "# 返回一个新的数据集，其中的样本已经被转换为模型所需的格式\n",
    "prepped_ds = dataset.with_transform(transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(batch):\n",
    "    # 定义一个数据整理函数，用于将一个批次的样本组合成一个批次的数据\n",
    "    return {\n",
    "        # 'pixel_values' 键对应的值是一个张量，包含了批次中所有样本的像素值\n",
    "        # 使用 torch.stack 将列表中的每个样本的 'pixel_values' 张量\n",
    "        # 堆叠成一个新的张量\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        \n",
    "        # 'labels' 键对应的值是一个张量，包含了批次中所有样本的标签\n",
    "        # 使用 torch.tensor 将列表中的每个样本的标签转换成一个新的张量\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    # 加载准确率评估指标\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    # 计算并获取准确率\n",
    "    accuracy = accuracy_metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)['accuracy']\n",
    "    \n",
    "    # 加载 F1 分数评估指标\n",
    "    f1_score_metric = evaluate.load(\"f1\")\n",
    "    # 计算并获取加权平均、微平均和宏平均的 F1 分数\n",
    "    weighted_f1_score = f1_score_metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids, average='weighted')[\"f1\"]\n",
    "    micro_f1_score = f1_score_metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids, average='micro')['f1']\n",
    "    macro_f1_score = f1_score_metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids, average='macro')[\"f1\"]\n",
    "    \n",
    "    # 加载召回率评估指标\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    # 计算并获取加权平均、微平均和宏平均的召回率\n",
    "    weighted_recall = recall_metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids, average='weighted')[\"recall\"]\n",
    "    micro_recall = recall_metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids, average='micro')[\"recall\"]\n",
    "    macro_recall = recall_metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids, average='macro')[\"recall\"]\n",
    "    \n",
    "    # 加载精确度评估指标\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    # 计算并获取加权平均、微平均和宏平均的精确度\n",
    "    weighted_precision = precision_metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids, average='weighted')[\"precision\"]\n",
    "    micro_precision = precision_metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids, average='micro')[\"precision\"]\n",
    "    macro_precision = precision_metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids, average='macro')[\"precision\"]\n",
    "    \n",
    "    # 返回所有评估指标的字典\n",
    "    return {\"accuracy\": accuracy, \n",
    "            \"Weighted F1\": weighted_f1_score,\n",
    "            \"Micro F1\": micro_f1_score,\n",
    "            \"Macro F1\": macro_f1_score,\n",
    "            \"Weighted Recall\": weighted_recall,\n",
    "            \"Micro Recall\": micro_recall,\n",
    "            \"Macro Recall\": macro_recall,\n",
    "            \"Weighted Precision\": weighted_precision,\n",
    "            \"Micro Precision\": micro_precision,\n",
    "            \"Macro Precision\": macro_precision\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "# 获取数据集中的标签名称列表\n",
    "labels = dataset['train'].features['label'].names\n",
    "\n",
    "# 从预训练的检查点加载 Vision Transformer (ViT) 模型用于图像分类\n",
    "# MODEL_CKPT 是之前定义的模型检查点名称\n",
    "# num_labels 指定了模型需要输出的标签数量\n",
    "# id2label 和 label2id 分别提供了从标签索引到标签名称和从标签名称到标签索引的映射\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    MODEL_CKPT,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ").to(DEVICE)  # 将模型移动到指定的设备上，例如 GPU 或 CPU\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=MODEL_CKPT + \"vegetables_classification_3.0\",  # 指定模型和训练日志的输出目录\n",
    "    remove_unused_columns=False,  # 是否移除数据集中未使用的列\n",
    "    num_train_epochs=NUM_OF_EPOCHS,  # 指定训练的轮数\n",
    "    evaluation_strategy=\"epoch\",  # 指定评估策略，每个 epoch 结束时进行评估\n",
    "    save_strategy=\"epoch\",  # 指定保存策略，每个 epoch 结束时保存模型\n",
    "    per_device_train_batch_size=BATCH_SIZE,  # 指定每个设备的训练批次大小\n",
    "    learning_rate=LEARNING_RATE,  # 指定学习率\n",
    "    report_to=REPORTS_TO,  # 指定报告的输出方式，例如 \"tensorboard\"\n",
    "    disable_tqdm=False,  # 是否禁用进度条\n",
    "    load_best_model_at_end=True,  # 训练结束时是否加载最佳模型\n",
    "    metric_for_best_model=\"Weighted F1\",  # 用于选择最佳模型的评估指标\n",
    "    logging_first_step=True,  # 是否在第一步记录日志\n",
    "    hub_private_repo=False,  ## 是否使用私有仓库推送到 Hugging Face Hub\n",
    "    push_to_hub=False  ## 是否推送模型到 Hugging Face Hub\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instantiate Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,  # 你之前定义的模型\n",
    "    args=args,  # 训练参数，之前通过 TrainingArguments 类定义\n",
    "    data_collator=data_collator,  # 数据整理函数，用于将多个样本组合成一个批次\n",
    "    compute_metrics=compute_metrics,  # 评估指标计算函数，用于在评估时计算指标\n",
    "    train_dataset=prepped_ds['train'],  # 训练数据集，之前通过 dataset.with_transform(transform) 预处理得到\n",
    "    eval_dataset=prepped_ds['test'],  # 测试数据集，用于评估模型性能\n",
    "    tokenizer=feature_extractor,  # 特征提取器，用于数据预处理\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1950' max='1950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1950/1950 29:43, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Weighted f1</th>\n",
       "      <th>Micro f1</th>\n",
       "      <th>Macro f1</th>\n",
       "      <th>Weighted recall</th>\n",
       "      <th>Micro recall</th>\n",
       "      <th>Macro recall</th>\n",
       "      <th>Weighted precision</th>\n",
       "      <th>Micro precision</th>\n",
       "      <th>Macro precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.620500</td>\n",
       "      <td>0.599933</td>\n",
       "      <td>0.883008</td>\n",
       "      <td>0.865571</td>\n",
       "      <td>0.883008</td>\n",
       "      <td>0.865781</td>\n",
       "      <td>0.883008</td>\n",
       "      <td>0.883008</td>\n",
       "      <td>0.883025</td>\n",
       "      <td>0.883724</td>\n",
       "      <td>0.883008</td>\n",
       "      <td>0.884047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.620500</td>\n",
       "      <td>0.267817</td>\n",
       "      <td>0.924791</td>\n",
       "      <td>0.922698</td>\n",
       "      <td>0.924791</td>\n",
       "      <td>0.922566</td>\n",
       "      <td>0.924791</td>\n",
       "      <td>0.924791</td>\n",
       "      <td>0.924383</td>\n",
       "      <td>0.937987</td>\n",
       "      <td>0.924791</td>\n",
       "      <td>0.938159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.838600</td>\n",
       "      <td>0.200774</td>\n",
       "      <td>0.933148</td>\n",
       "      <td>0.931522</td>\n",
       "      <td>0.933148</td>\n",
       "      <td>0.931365</td>\n",
       "      <td>0.933148</td>\n",
       "      <td>0.933148</td>\n",
       "      <td>0.932716</td>\n",
       "      <td>0.943474</td>\n",
       "      <td>0.933148</td>\n",
       "      <td>0.943631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.838600</td>\n",
       "      <td>0.143789</td>\n",
       "      <td>0.955432</td>\n",
       "      <td>0.954122</td>\n",
       "      <td>0.955432</td>\n",
       "      <td>0.953903</td>\n",
       "      <td>0.955432</td>\n",
       "      <td>0.955432</td>\n",
       "      <td>0.954938</td>\n",
       "      <td>0.964140</td>\n",
       "      <td>0.955432</td>\n",
       "      <td>0.964239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.838600</td>\n",
       "      <td>0.130252</td>\n",
       "      <td>0.963788</td>\n",
       "      <td>0.962987</td>\n",
       "      <td>0.963788</td>\n",
       "      <td>0.962742</td>\n",
       "      <td>0.963788</td>\n",
       "      <td>0.963788</td>\n",
       "      <td>0.963272</td>\n",
       "      <td>0.967855</td>\n",
       "      <td>0.963788</td>\n",
       "      <td>0.967944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.128100</td>\n",
       "      <td>0.117727</td>\n",
       "      <td>0.966574</td>\n",
       "      <td>0.965578</td>\n",
       "      <td>0.966574</td>\n",
       "      <td>0.965326</td>\n",
       "      <td>0.966574</td>\n",
       "      <td>0.966574</td>\n",
       "      <td>0.966049</td>\n",
       "      <td>0.974158</td>\n",
       "      <td>0.966574</td>\n",
       "      <td>0.974229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.128100</td>\n",
       "      <td>0.101958</td>\n",
       "      <td>0.969359</td>\n",
       "      <td>0.969622</td>\n",
       "      <td>0.969359</td>\n",
       "      <td>0.969359</td>\n",
       "      <td>0.969359</td>\n",
       "      <td>0.969359</td>\n",
       "      <td>0.968827</td>\n",
       "      <td>0.973309</td>\n",
       "      <td>0.969359</td>\n",
       "      <td>0.973383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.039700</td>\n",
       "      <td>0.106312</td>\n",
       "      <td>0.969359</td>\n",
       "      <td>0.969137</td>\n",
       "      <td>0.969359</td>\n",
       "      <td>0.968876</td>\n",
       "      <td>0.969359</td>\n",
       "      <td>0.969359</td>\n",
       "      <td>0.968827</td>\n",
       "      <td>0.974580</td>\n",
       "      <td>0.969359</td>\n",
       "      <td>0.974650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.039700</td>\n",
       "      <td>0.103100</td>\n",
       "      <td>0.969359</td>\n",
       "      <td>0.969137</td>\n",
       "      <td>0.969359</td>\n",
       "      <td>0.968876</td>\n",
       "      <td>0.969359</td>\n",
       "      <td>0.969359</td>\n",
       "      <td>0.968827</td>\n",
       "      <td>0.974580</td>\n",
       "      <td>0.969359</td>\n",
       "      <td>0.974650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.039700</td>\n",
       "      <td>0.103168</td>\n",
       "      <td>0.969359</td>\n",
       "      <td>0.969137</td>\n",
       "      <td>0.969359</td>\n",
       "      <td>0.968876</td>\n",
       "      <td>0.969359</td>\n",
       "      <td>0.969359</td>\n",
       "      <td>0.968827</td>\n",
       "      <td>0.974580</td>\n",
       "      <td>0.969359</td>\n",
       "      <td>0.974650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages/PIL/Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages/PIL/Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages/PIL/Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages/PIL/Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages/PIL/Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages/PIL/Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the module from /home/yuan314/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--precision/4e7f439a346715f68500ce6f2be82bf3272abd3f20bdafd203a2c4f85b61dd5f (last modified on Wed Mar 27 16:31:20 2024) since it couldn't be found locally at evaluate-metric--precision, or remotely on the Hugging Face Hub.\n",
      "/home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages/PIL/Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages/PIL/Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages/PIL/Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/yuan314/anaconda3/envs/vit/lib/python3.9/site-packages/PIL/Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         10.0\n",
      "  total_flos               = 2248781788GF\n",
      "  train_loss               =       0.2648\n",
      "  train_runtime            =   0:29:45.07\n",
      "  train_samples_per_second =        17.45\n",
      "  train_steps_per_second   =        1.092\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)#记录训练指标\n",
    "trainer.save_metrics(\"train\", train_results.metrics)#保存训练指标\n",
    "trainer.save_state()#保存训练状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch: 表示训练经过的总轮数（epochs）。在这个例子中，训练进行了 3 个 epochs。\n",
    "\n",
    "total_flos: 表示训练过程中执行的浮点运算总数，以浮点运算次数（FLOPs）计算。在这个例子中，总共执行了约 3753.28 万亿次浮点运算。\n",
    "\n",
    "train_loss: 表示训练过程中的平均损失（loss）。在这个例子中，最终的平均损失是 0.0134。\n",
    "\n",
    "train_runtime: 表示训练的总运行时间。在这个例子中，训练总共花费了 11 分 44.25 秒。\n",
    "\n",
    "train_samples_per_second: 表示每秒处理的样本数。在这个例子中，每秒处理了约 63.898 个样本。\n",
    "\n",
    "train_steps_per_second: 表示每秒执行的训练步骤（或批次）数。在这个例子中，每秒执行了约 3.996 步训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       10.0\n",
      "  eval_Macro F1           =     0.9694\n",
      "  eval_Macro Precision    =     0.9734\n",
      "  eval_Macro Recall       =     0.9688\n",
      "  eval_Micro F1           =     0.9694\n",
      "  eval_Micro Precision    =     0.9694\n",
      "  eval_Micro Recall       =     0.9694\n",
      "  eval_Weighted F1        =     0.9696\n",
      "  eval_Weighted Precision =     0.9733\n",
      "  eval_Weighted Recall    =     0.9694\n",
      "  eval_accuracy           =     0.9694\n",
      "  eval_loss               =      0.102\n",
      "  eval_runtime            = 0:00:30.88\n",
      "  eval_samples_per_second =     11.624\n",
      "  eval_steps_per_second   =      1.457\n"
     ]
    }
   ],
   "source": [
    "# 使用测试数据集对模型进行评估\n",
    "metrics = trainer.evaluate(prepped_ds['test'])\n",
    "\n",
    "# 将评估结果记录到日志中\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "\n",
    "# 将评估指标保存到文件中\n",
    "trainer.save_metrics(\"eval\", metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些是你的模型在评估（测试）阶段的性能指标。下面是对每个指标的简要解释：\n",
    "\n",
    "- `epoch`: 训练完成的轮数（epochs）。\n",
    "\n",
    "- `eval_Macro F1`: 宏平均 F1 分数，计算每个类别的 F1 分数然后取平均。\n",
    "\n",
    "- `eval_Macro Precision`: 宏平均精确度，计算每个类别的精确度然后取平均。\n",
    "\n",
    "- `eval_Macro Recall`: 宏平均召回率，计算每个类别的召回率然后取平均。\n",
    "\n",
    "- `eval_Micro F1`: 微平均 F1 分数，先计算总的真正例、假正例和假负例，然后计算 F1 分数。\n",
    "\n",
    "- `eval_Micro Precision`: 微平均精确度，先计算总的真正例和假正例，然后计算精确度。\n",
    "\n",
    "- `eval_Micro Recall`: 微平均召回率，先计算总的真正例和假负例，然后计算召回率。\n",
    "\n",
    "- `eval_Weighted F1`: 加权平均 F1 分数，根据每个类别的样本数加权计算 F1 分数。\n",
    "\n",
    "- `eval_Weighted Precision`: 加权平均精确度，根据每个类别的样本数加权计算精确度。\n",
    "\n",
    "- `eval_Weighted Recall`: 加权平均召回率，根据每个类别的样本数加权计算召回率。\n",
    "\n",
    "- `eval_accuracy`: 准确率，正确分类的样本数占总样本数的比例。\n",
    "\n",
    "- `eval_loss`: 评估阶段的平均损失。\n",
    "\n",
    "- `eval_runtime`: 评估阶段的运行时间。\n",
    "\n",
    "- `eval_samples_per_second`: 每秒处理的样本数。\n",
    "\n",
    "- `eval_steps_per_second`: 每秒执行的步骤（批次）数。\n",
    "\n",
    "这些指标提供了模型在测试数据集上的性能概览，可以帮助你了解模型的泛化能力和预测准确性。在这个例子中，模型的表现非常好，几乎所有指标都接近 1.0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Push Model to Hub (My Profile!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"finetuned_from\" : model.config._name_or_path,\n",
    "    \"tasks\" : \"image-classification\",\n",
    "    \"tags\" : [\"image-classification\"],\n",
    "}\n",
    "\n",
    "if args.push_to_hub:\n",
    "    trainer.push_to_hub(\"All Dunn!!!\")\n",
    "else:\n",
    "    trainer.create_model_card(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes & Other Takeaways\n",
    "****\n",
    "- Wow, I have never had a model that perfect results!\n",
    "- I am not sure that I would want to use the third epoch version of the project. It is perfect, but there is concern about overtraining.\n",
    "- 哇，我从来没有一个模型能达到如此完美的效果！\n",
    "- 我不确定我是否想使用该项目的第三个历元版本。这是完美的，但人们担心过度训练。\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: tomato\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "image_path = 'test/xihongshi.png'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "#转换颜色空间：OpenCV 默认加载图片的颜色空间是 BGR，\n",
    "#而通常模型需要的输入颜色空间是 RGB，因此你需要进行转换：\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#预处理图片\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "#进行预测：然后，你可以将预处理后的图片输入到模型中，进行预测：\n",
    "with torch.no_grad():  # 确保不计算梯度，节省计算资源\n",
    "    outputs = model(**inputs.to(DEVICE))  # 将输入数据移动到相同的设备上\n",
    "    \n",
    "#解析预测结果：模型的输出通常是一个 logits 或者 probabilities 的张量。\n",
    "#你可以使用 torch.argmax 来获取最可能的类别索引，然后将其转换成类别名称：\n",
    "pred_label_idx = torch.argmax(outputs.logits, dim=1).item()  # 获取最可能的类别索引\n",
    "pred_label = labels[pred_label_idx]  # 将索引转换成类别名称\n",
    "\n",
    "print(f'Predicted label: {pred_label}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0263, -0.7184, -0.6585, -0.3652,  0.0969, -0.6789, -0.0830,  0.1112,\n",
       "          0.2340, -0.4124, -0.5306,  0.5624, -0.4493, -0.4274, -0.5315, -0.2337,\n",
       "         -0.6679, -0.8307, -0.4627, -0.0928, -0.2848, -0.4195, -0.4754, -0.2980,\n",
       "         -0.5075, -0.4875,  0.0463, -0.0182, -0.2832, -0.7199, -0.5597, -0.1545,\n",
       "         -0.0285,  8.3290, -0.4389, -0.3395]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fruit-and-vegetable-image-detection-vit.ipynb  new\r\n",
      "fruit_vegetable_image_detection\t\t       test\r\n",
      "google\t\t\t\t\t       Untitled.ipynb\r\n",
      "image\t\t\t\t\t       Vegetables_ViT2.ipynb\r\n",
      "MiSans-Normal.ttf\t\t\t       Vegetables_ViT.ipynb\r\n",
      "mlruns\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xiangjiao.png  xihongshi  xihongshi.png\r\n"
     ]
    }
   ],
   "source": [
    "!ls test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Label            Probability\n",
      "  1: banana          0.99571556\n",
      " 19: mango           0.00029658\n",
      " 24: peas            0.00023871\n",
      " 17: lemon           0.00017473\n",
      "  2: beetroot        0.00015865\n",
      "  3: bell pepper     0.00015862\n",
      "  9: corn            0.00015647\n",
      " 21: orange          0.00014761\n",
      " 25: pineapple       0.00014714\n",
      " 16: kiwi            0.00014189\n",
      " 15: jalepeno        0.00013929\n",
      " 28: raddish         0.00013864\n",
      " 31: sweetcorn       0.00013553\n",
      "  5: capsicum        0.00013224\n",
      " 32: sweetpotato     0.00012650\n",
      " 29: soy beans       0.00011763\n",
      " 18: lettuce         0.00011504\n",
      " 26: pomegranate     0.00011380\n",
      "  0: apple           0.00011268\n",
      " 22: paprika         0.00011084\n",
      " 12: garlic          0.00010987\n",
      " 13: ginger          0.00010980\n",
      "  6: carrot          0.00010641\n",
      " 20: onion           0.00009548\n",
      " 23: pear            0.00009530\n",
      "  7: cauliflower     0.00009394\n",
      " 27: potato          0.00009240\n",
      " 30: spinach         0.00009165\n",
      " 35: watermelon      0.00008826\n",
      " 10: cucumber        0.00008779\n",
      " 14: grapes          0.00008741\n",
      "  4: cabbage         0.00008692\n",
      " 33: tomato          0.00007947\n",
      " 11: eggplant        0.00007392\n",
      "  8: chilli pepper   0.00007063\n",
      " 34: turnip          0.00005266\n",
      "Predicted label: 1.banana\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# 加载模型\n",
    "model_path = 'google/vit-base-patch16-224-in21kvegetables_classification_3.0/checkpoint-1950'\n",
    "model = ViTForImageClassification.from_pretrained(model_path)\n",
    "#载预训练的图像处理器\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(model_path)\n",
    "\n",
    "# 加载图片\n",
    "image_path = 'test/xiangjiao.png'\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# 预处理图片\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# 使用模型进行预测\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# 获取预测结果\n",
    "pred_label_idx = torch.argmax(outputs.logits, dim=1).item()\n",
    "pred_label = model.config.id2label[pred_label_idx]\n",
    "\n",
    "\n",
    "# 假设 outputs 是模型的输出，包含 logits\n",
    "logits = outputs.logits\n",
    "\n",
    "# 使用 softmax 函数将 logits 转换为概率\n",
    "probs = softmax(logits, dim=1)\n",
    "\n",
    "# 获取每个类别的概率及其索引\n",
    "prob_list = [(p.item(), idx) for idx, p in enumerate(probs[0])]\n",
    "\n",
    "# 按概率从大到小排序\n",
    "sorted_probs = sorted(prob_list, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# 打印表头\n",
    "print(f\"   {'Label':<16}{'Probability':>12}\")\n",
    "\n",
    "# 打印排序后的概率及对应的标签索引\n",
    "for prob, idx in sorted_probs:\n",
    "    print(f\"{idx:>3}: {model.config.id2label[idx]:<14}{prob:>12.8f}\")\n",
    "\n",
    "\n",
    "# 打印识别结果\n",
    "print(f'Predicted label: {pred_label_idx}.{pred_label}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'apple',\n",
       " 1: 'banana',\n",
       " 10: 'cucumber',\n",
       " 11: 'eggplant',\n",
       " 12: 'garlic',\n",
       " 13: 'ginger',\n",
       " 14: 'grapes',\n",
       " 15: 'jalepeno',\n",
       " 16: 'kiwi',\n",
       " 17: 'lemon',\n",
       " 18: 'lettuce',\n",
       " 19: 'mango',\n",
       " 2: 'beetroot',\n",
       " 20: 'onion',\n",
       " 21: 'orange',\n",
       " 22: 'paprika',\n",
       " 23: 'pear',\n",
       " 24: 'peas',\n",
       " 25: 'pineapple',\n",
       " 26: 'pomegranate',\n",
       " 27: 'potato',\n",
       " 28: 'raddish',\n",
       " 29: 'soy beans',\n",
       " 3: 'bell pepper',\n",
       " 30: 'spinach',\n",
       " 31: 'sweetcorn',\n",
       " 32: 'sweetpotato',\n",
       " 33: 'tomato',\n",
       " 34: 'turnip',\n",
       " 35: 'watermelon',\n",
       " 4: 'cabbage',\n",
       " 5: 'capsicum',\n",
       " 6: 'carrot',\n",
       " 7: 'cauliflower',\n",
       " 8: 'chilli pepper',\n",
       " 9: 'corn'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label\n",
    "#pred_label_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(outputs.logits, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "原始单元格格式",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc72b274efffb8cc120df26a7300ddf89b50ec5391b64e9e8132c8a950ce965e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
